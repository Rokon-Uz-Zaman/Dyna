{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reserve TAM_Keras_h5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNcXn5WzWLdvybOIQmnXT0H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rokon-Uz-Zaman/Dyna/blob/master/Reserve_TAM_Keras_h5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rUUTtPlM73V"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[link text](https://medium.datadriveninvestor.com/multi-class-image-classification-in-teachable-machine-and-its-real-time-detection-with-opencv-282a1409006f)"
      ],
      "metadata": {
        "id": "8WPgr3hGND51"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At first, Go to Teachable Machine and Choose a new Image Project. Add image samples in the various classes as required and Choose Train Model. Now, Use the Preview feature to verify that your model is accurate. Choose Export the model â†’ Select Tensorflow. Check Model conversion type as Keras and download your model into your local computer. A zip file named converted_keras is downloaded. Unzip the file contents into a working directory.\n",
        "The working directory now contains labels.txt (which contains information about classes we have)and keras_model.h5 (which is the ML model trained in tensorflow by Teachable machine)."
      ],
      "metadata": {
        "id": "YzERBipaNkIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generates a dict to store the labels(keys) and its names(values)\n",
        "\n",
        "def gen_labels():\n",
        "        labels = {}\n",
        "        with open(\"labels.txt\", \"r\") as label:\n",
        "            text = label.read()\n",
        "            lines = text.split(\"\\n\")\n",
        "            print(lines)\n",
        "            for line in lines[0:-1]:\n",
        "                    hold = line.split(\" \", 1)\n",
        "                    labels[hold[0]] = hold[1]\n",
        "        return labels"
      ],
      "metadata": {
        "id": "kMN4eXDANHda"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_labels()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHC_fHyrRA8o",
        "outputId": "9239621e-c2ce-4ac5-8088-567c523d1aef"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['0 Apple', '1 Jack', '']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'0': 'Apple', '1': 'Jack'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras\n",
        "import numpy as np\n",
        "import cv2\n",
        "#from process_labels import gen_labels\n",
        "\n",
        "# Disable scientific notation for clarity\n",
        "np.set_printoptions(suppress=True)\n",
        "#image = cv2.VideoCapture(0)\n",
        "# Load the model\n",
        "model = tensorflow.keras.models.load_model('keras_model.h5')\n",
        "\n",
        "\"\"\"\n",
        "Create the array of the right shape to feed into the keras model\n",
        "The 'length' or number of images you can put into the array is\n",
        "determined by the first position in the shape tuple, in this case 1.\"\"\"\n",
        "data = np.ndarray(shape=(1, 224, 224, 3), dtype=np.float32)\n",
        "# A dict that stores the labels\n",
        "labels = gen_labels()\n",
        "\n",
        "\n",
        "\n",
        "##############Video writer\n",
        "image= cv2.VideoCapture(\"/content/Apple.mp4\")\n",
        "\n",
        "frameWidth=int(image.get(3))\n",
        "frameHeight= int(image.get(4))\n",
        "size=(frameWidth,frameHeight)\n",
        "#result = cv2.VideoWriter('filename.avi',cv2.VideoWriter_fourcc(*'MJPG'),10,size)\n",
        "out=cv2.VideoWriter('outputfilefruit.avi',cv2.VideoWriter_fourcc(*'MJPG'),10,size)\n",
        "\n",
        "while True:\n",
        "    # Choose a suitable font\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    ret, frame = image.read()\n",
        "    frame = cv2.flip(frame, 1)\n",
        "    # In case the image is not read properly\n",
        "    if not ret:\n",
        "        continue\n",
        "    # Draw a rectangle, in the frame\n",
        "    frame = cv2.rectangle(frame, (220, 80), (530, 360), (0, 0, 255), 3)\n",
        "    # Draw another rectangle in which the image to labelled is to be shown.\n",
        "    frame2 = frame[80:360, 220:530]\n",
        "    # resize the image to a 224x224 with the same strategy as in TM2:\n",
        "    # resizing the image to be at least 224x224 and then cropping from the center\n",
        "    frame2 = cv2.resize(frame2, (224, 224))\n",
        "    # turn the image into a numpy array\n",
        "    image_array = np.asarray(frame2)\n",
        "    # Normalize the image\n",
        "    normalized_image_array = (image_array.astype(np.float32) / 127.0) - 1\n",
        "    # Load the image into the array\n",
        "    data[0] = normalized_image_array\n",
        "    pred = model.predict(data)\n",
        "    result = np.argmax(pred[0])\n",
        "\n",
        "    # Print the predicted label into the screen.\n",
        "    cv2.putText(frame,  \"Label : \" +\n",
        "                labels[str(result)], (280, 400), font, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
        "\n",
        "    # Exit, when 'q' is pressed on the keyboard\n",
        "    if cv2.waitKey(1) and 0xff == ord('q'):\n",
        "        exit = True\n",
        "        break\n",
        "    # Show the frame   \n",
        "    #cv2.imshow('Frame', frame)\n",
        "    out.write(frame) \n",
        "\n",
        "image.release()\n",
        "#cv2.destroyAllWindows()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "MT_PnkeQO6y7",
        "outputId": "23c66ac1-17df-456f-98f5-cdc8c4c43949"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "['0 Apple', '1 Jack', '']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-bffcf77cfb07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# Choose a suitable font\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mfont\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFONT_HERSHEY_SIMPLEX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# In case the image is not read properly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}